{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains the code to generate the simulated data based on the input hyperparameter values. The simulated data is stored within a folder called \"simulated_data\" and within sub-folders named according to the hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 50\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(words):\n",
    "    # returns the frequency of each word in a given list of words\n",
    "    cnt = Counter(words)\n",
    "    freq = list(cnt.values())\n",
    "    words = list(cnt.keys())\n",
    "    return freq, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(alpha, beta, num_patients, num_tokens, vocab_size, num_topics=num_topics):\n",
    "    \n",
    "    mixehr_sim_data = {'patId':[], 'typeId':[], 'pheId':[], 'stateId':[], 'freq':[]} # dictionary for making mxr_data\n",
    "    mixehr_sim_meta = {'typeId':[], 'pheId':[], 'statecnt':[]} # dictionary for making mxr_meta\n",
    "    \n",
    "    words_by_topic = {} # phi matrix\n",
    "    \n",
    "    # sampling patients from topic mixture. [num_patients x K] \n",
    "    patients_by_topics = np.random.dirichlet([alpha]*num_topics, size=num_patients) # theta matrix?\n",
    "    \n",
    "    # sampling K topic probabilities. [vocab_size x K]\n",
    "    for ind, beta_t in enumerate(beta): # for each note-type, we sample a phi matrix\n",
    "        topic_probs = np.random.dirichlet([beta_t]*vocab_size, size=num_topics)\n",
    "        words_by_topic[ind+1] = topic_probs\n",
    "        \n",
    "    words_picked_from_vocab = {1: set(), 2: set(), 3: set(), 4: set()}\n",
    "    \n",
    "    for patient in tqdm(range(1, num_patients+1)): # patId has to start from 1 and not 0\n",
    "\n",
    "#           for running the experiments of simulating missing note types, uncomment the following lines\n",
    "#           drop = np.random.random() # when we want to randomly drop one of the note types for missing types\n",
    "#           if drop > 0.5:\n",
    "#               types = [1]\n",
    "#           else:\n",
    "#               types = [2]\n",
    "            # populating each patient record with sampled tokens/words\n",
    "            patids = []\n",
    "            typeids = []\n",
    "            pheids = []\n",
    "            stateids = []\n",
    "            freq = []\n",
    "            sampled_words = []        \n",
    "            types = [1,2,3,4]\n",
    "    \n",
    "            for type_id in types: # for each note-type (i.e doctor's note and nurse's notes)\n",
    "\n",
    "                beta_t = beta[type_id] # both types have same value of beta_t = 0.1 for now\n",
    "                words_from_type = [] # e.g words sampled from nurses' notes & vocab\n",
    "\n",
    "                while len(set(words_from_type)) < int(np.ceil(num_tokens/len(types))): # for each patient we sample 'num_tokens' number of tokens \n",
    "                    # we randomly pick one topic to sample words from the pat-topic mixture of given patient\n",
    "                    picked_topic = np.argmax(np.random.multinomial(1, patients_by_topics[patient-1], size=1)[0])\n",
    "                    picked_topic_dist = words_by_topic[type_id][picked_topic] # distbn over vocab\n",
    "\n",
    "                    sampled_word = np.argmax(np.random.multinomial(1, picked_topic_dist, size=1)[0]) # we sample a word\n",
    "                    words_from_type.append(sampled_word+1)\n",
    "                    \n",
    "                    words_picked_from_vocab[type_id].add(sampled_word)\n",
    "                \n",
    "                sampled_words.extend(words_from_type)\n",
    "                          \n",
    "                # we calculate the frequency of words sampled for the last column in mxr_data\n",
    "                sampled_words_freq, words = frequency(sampled_words) \n",
    "\n",
    "                num_lines = len(sampled_words_freq) # for each sampled word, we need to repeat patId and typeId in mxr_data\n",
    "\n",
    "                patids.extend([patient]*num_lines)\n",
    "                typeids.extend([type_id]*num_lines)\n",
    "                pheids.extend(words)\n",
    "                stateids.extend([0]*num_lines) # no state counts so always 0\n",
    "                freq.extend(sampled_words_freq)\n",
    "\n",
    "                # entering rows of mxr_meta file\n",
    "                mixehr_sim_meta['typeId'].extend([type_id]*num_lines)\n",
    "                mixehr_sim_meta['pheId'].extend(words)\n",
    "                mixehr_sim_meta['statecnt'].extend([1]*num_lines)\n",
    "\n",
    "                # entering rows of mxr_data file\n",
    "                mixehr_sim_data['patId'].extend(patids)\n",
    "                mixehr_sim_data['typeId'].extend(typeids)\n",
    "                mixehr_sim_data['pheId'].extend(pheids)\n",
    "                mixehr_sim_data['stateId'].extend(stateids)\n",
    "                mixehr_sim_data['freq'].extend(freq)\n",
    "\n",
    "                # entering remaining words into the meta file\n",
    "                remaining_words = set(range(1,vocab_size+1)) - words_picked_from_vocab[type_id]\n",
    "\n",
    "                for rem_word in remaining_words:\n",
    "                    mixehr_sim_meta['typeId'].append(type_id)\n",
    "                    mixehr_sim_meta['pheId'].append(rem_word)\n",
    "                    mixehr_sim_meta['statecnt'].append(1)       \n",
    "    \n",
    "    mxr_data = pd.DataFrame(data=mixehr_sim_data)\n",
    "    mxr_meta = pd.DataFrame(data=mixehr_sim_meta)\n",
    "    mxr_meta = mxr_meta.sort_values(by=['typeId','pheId'])\n",
    "    \n",
    "    return mxr_data, mxr_meta, words_by_topic, patients_by_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters\n",
    "alpha = 0.01 # hyparparameter for the patient-topic mixture\n",
    "beta_t = {1: 0.01,\n",
    "          2: 0.01,\n",
    "          3: 0.01,\n",
    "          4: 0.01} # hyperparameter for the topic-word mixture\n",
    "num_topics = 50 # K\n",
    "num_patients = [1000, 4000, 8000] # D\n",
    "num_tokens = [1000, 1500, 2000] # M\n",
    "vocab_sizes = [2000, 2500, 3000] # V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_less_than_5(data, meta):\n",
    "    words_to_remove = {}\n",
    "    for type_id in [1, 2]:\n",
    "        dd = data[data['typeId'] == type_id]\n",
    "        mm = meta[meta['typeId'] == type_id]\n",
    "        words_to_remove[type_id] = []\n",
    "        for grp in dd[['patId','pheId']].groupby('pheId'):\n",
    "            patients = grp[1]['patId'].values\n",
    "            if len(np.unique(patients)) < 5:\n",
    "                #print(np.unique(patients))\n",
    "                words_to_remove[type_id].append(grp[0])\n",
    "        data = data.drop(dd[dd['pheId'].isin(words_to_remove[type_id])].index,axis=0)\n",
    "        #print(words_to_remove[type_id])\n",
    "        meta = meta.drop(mm[mm['pheId'].isin(words_to_remove[type_id])].index, axis=0)\n",
    "    return data, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulated_data/4000pats/300tokens/2500vocab/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d372b5097e734b9b88b6907d89196a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run the simulations for different hyperparameter values\n",
    "for patients in num_patients:\n",
    "    for tokens in num_tokens:\n",
    "        for vocab in vocab_sizes:\n",
    "            file_name = \"_\"+str(patients)+\"pats_\"+str(tokens)+\"tokens_\"+str(vocab)+\"vocab\"\n",
    "            folder_name = 'simulated_data/'+ str(patients)+\"pats/\"+str(tokens)+\"tokens/\"+str(vocab)+\"vocab/\"\n",
    "\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "\n",
    "            print(folder_name)   \n",
    "            mxr_data, mxr_meta, phi, theta = simulation(alpha,beta_t, num_patients=patients, num_tokens=tokens, vocab_size=vocab)\n",
    "            mxr_data.to_csv(folder_name+\"sim_mxr_data\"+file_name+\".txt\", index=False,header=None, sep=' ')\n",
    "            mxr_meta.to_csv(folder_name+\"sim_mxr_meta\"+file_name+\".txt\",index=False,header=None, sep=' ')\n",
    "            pkl.dump(phi, open(folder_name+\"sim_phi\"+file_name+\".pkl\",\"wb\"))\n",
    "            pkl.dump(theta, open(folder_name+\"sim_theta\"+file_name+\".csv\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
