{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains the code to create a baseline data and meta file for training the heterogenous_ehr model. The baseline considers the same words from different note-types as the same and just adds up their frequency. This is the LDA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline1(data, meta):\n",
    "    # The first baseline considers the same word from different note types the same. \n",
    "    # So it just use the total count for the same word across notes.\n",
    "    mixehr_sim_data = {'patId':[], 'typeId':[], 'pheId':[], 'stateId':[], 'freq':[]} # dictionary for making mxr_data\n",
    "    mixehr_sim_meta = {'typeId':[], 'pheId':[], 'statecnt':[]} # dictionary for making mxr_meta\n",
    "    vocab_size = 2500\n",
    "    words_picked_from_vocab = set()\n",
    "    pairs = set()\n",
    "    for grp in tqdm(data.groupby(2)):\n",
    "        tt = grp[1]\n",
    "\n",
    "        for g in tt.groupby(0):\n",
    "            if len(g[1]) > 1:\n",
    "                freq = sum(g[1][4])\n",
    "            else:\n",
    "                freq = 1\n",
    "            type_id = 1\n",
    "            word = g[1][2].values[0]\n",
    "\n",
    "            patid = g[1][0].values[0]\n",
    "            stateid = 0\n",
    "            words_picked_from_vocab.add(word)\n",
    "\n",
    "            # entering rows of mxr_meta file\n",
    "            if (type_id, word) not in pairs:\n",
    "                mixehr_sim_meta['typeId'].append(type_id)\n",
    "                mixehr_sim_meta['pheId'].append(word)\n",
    "                mixehr_sim_meta['statecnt'].append(1)\n",
    "                pairs.add((type_id, word))\n",
    "\n",
    "            # entering rows of mxr_data file\n",
    "            mixehr_sim_data['patId'].append(patid)\n",
    "            mixehr_sim_data['typeId'].append(type_id)\n",
    "            mixehr_sim_data['pheId'].append(word)\n",
    "            mixehr_sim_data['stateId'].append(stateid)\n",
    "            mixehr_sim_data['freq'].append(freq)\n",
    "\n",
    "    # entering remaining words into the meta file\n",
    "    remaining_words = set(range(1,vocab_size+1)) - words_picked_from_vocab\n",
    "\n",
    "    for rem_word in remaining_words:\n",
    "        mixehr_sim_meta['typeId'].append(type_id)\n",
    "        mixehr_sim_meta['pheId'].append(rem_word)\n",
    "        mixehr_sim_meta['statecnt'].append(1)       \n",
    "    \n",
    "    mxr_data = pd.DataFrame(data=mixehr_sim_data)\n",
    "    mxr_meta = pd.DataFrame(data=mixehr_sim_meta)\n",
    "    mxr_meta = mxr_meta.sort_values(by=['typeId','pheId'])\n",
    "    mxr_data = mxr_data.sort_values(by='patId')\n",
    "    \n",
    "    return mxr_data, mxr_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c982046378c04a30a392e97dc8256eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b25f2d491947dbb2b3eeadac93e5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for type_id in [2, 4]:\n",
    "    file_d = \"simulated_data/4000pats/300tokens/2500vocab/sim_mxr_data_type\"+str(type_id)+\"_new_4000pats_300tokens_2500vocab.txt\"\n",
    "    file_m = \"simulated_data/4000pats/300tokens/2500vocab/sim_mxr_meta_type\"+str(type_id)+\"_new_4000pats_300tokens_2500vocab.txt\"\n",
    "    data = pd.read_csv(file_d, header=None, sep=' ')\n",
    "    meta = pd.read_csv(file_m, header=None, sep=' ')\n",
    "    d1, m1 = baseline1(data, meta)\n",
    "    \n",
    "    d1.to_csv(\"simulated_data/4000pats/300tokens/2500vocab/sim_mxr_data_type\"+str(type_id)+\"_new_b1_4000pats_300tokens_2500vocab.txt\",header=None, index=False,sep=' ')\n",
    "    m1.to_csv(\"simulated_data/4000pats/300tokens/2500vocab/sim_mxr_meta_type\"+str(type_id)+\"_new_b1_4000pats_300tokens_2500vocab.txt\",header=None, index=False,sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
